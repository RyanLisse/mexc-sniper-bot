/**
 * Chaos Engineering Framework
 * Fault injection and resilience testing for the AI trading system
 */

import { performance } from 'node:perf_hooks'
import { randomBytes } from 'crypto'
import { BrowserCompatibleEventEmitter } from "@/src/lib/browser-compatible-events"

// ===================== CHAOS ENGINEERING TYPES =====================

export interface ChaosExperiment {
  id: string
  name: string
  description: string
  hypothesis: string
  blastRadius: 'single_agent' | 'agent_group' | 'system_wide' | 'external_dependencies'
  severity: 'low' | 'medium' | 'high' | 'critical'
  duration: number // milliseconds
  faults: ChaosFault[]
  successCriteria: ChaosSuccessCriteria
  rollbackTriggers: RollbackTrigger[]
  safeguards: ChaosSafeguard[]
  metrics: string[]
}

export interface ChaosFault {
  id: string
  type: 'latency' | 'failure' | 'resource_exhaustion' | 'network_partition' | 'data_corruption' | 'memory_leak' | 'cpu_spike' | 'disk_full' | 'dependency_failure' | 'timeout'
  target: ChaosTarget
  parameters: ChaosFaultParameters
  duration: number
  intensity: number // 0-100 percentage
  injectionPattern: 'constant' | 'intermittent' | 'gradual' | 'spike' | 'random'
  conditions: ChaosCondition[]
}

export interface ChaosTarget {
  type: 'agent' | 'service' | 'database' | 'api_endpoint' | 'network' | 'memory' | 'cpu' | 'disk'
  identifier: string
  scope: 'specific' | 'group' | 'all'
  filters?: Record<string, unknown>
}

export interface ChaosFaultParameters {
  latencyMs?: number
  errorRate?: number // 0-100 percentage
  memoryLeakRate?: number // MB per second
  cpuUsagePercent?: number
  diskUsagePercent?: number
  timeoutMs?: number
  responseCode?: number
  errorMessage?: string
  corruptionType?: 'data' | 'config' | 'state'
  networkPartitionTargets?: string[]
}

export interface ChaosCondition {
  metric: string
  operator: '>' | '<' | '=' | '>=' | '<=' | '!='
  threshold: number
  evaluationPeriod: number // milliseconds
}

export interface ChaosSuccessCriteria {
  systemAvailability: number // percentage
  maxResponseTimeIncrease: number // percentage
  maxErrorRateIncrease: number // percentage
  recoveryTime: number // milliseconds
  dataIntegrity: boolean
  agentCoordination: boolean
  emergencyProtocols: boolean
}

export interface RollbackTrigger {
  condition: ChaosCondition
  action: 'stop_experiment' | 'reduce_intensity' | 'activate_safeguard'
  priority: 'low' | 'medium' | 'high' | 'emergency'
}

export interface ChaosSafeguard {
  id: string
  type: 'circuit_breaker' | 'rate_limiter' | 'fallback' | 'isolation' | 'emergency_stop'
  trigger: ChaosCondition
  action: string
  autoActivate: boolean
}

export interface ChaosExperimentResult {
  experimentId: string
  startTime: number
  endTime: number
  duration: number
  success: boolean
  faultResults: ChaosFaultResult[]
  systemMetrics: ChaosSystemMetrics
  observations: ChaosObservation[]
  rollbackEvents: RollbackEvent[]
  safeguardActivations: SafeguardActivation[]
  lessons: string[]
  recommendations: string[]
}

export interface ChaosFaultResult {
  faultId: string
  injectionStartTime: number
  injectionEndTime: number
  actualIntensity: number
  targetCompliance: number // percentage of how well the fault was injected
  impactMetrics: Record<string, number>
  unexpectedEffects: string[]
}

export interface ChaosSystemMetrics {
  availability: number
  responseTime: {
    baseline: number
    duringChaos: number
    increase: number
  }
  errorRate: {
    baseline: number
    duringChaos: number
    increase: number
  }
  throughput: {
    baseline: number
    duringChaos: number
    decrease: number
  }
  resourceUsage: {
    memory: { baseline: number, peak: number }
    cpu: { baseline: number, peak: number }
    network: { baseline: number, peak: number }
  }
  recoveryMetrics: {
    detectionTime: number
    mitigationTime: number
    fullRecoveryTime: number
  }
}

export interface ChaosObservation {
  timestamp: number
  category: 'system_behavior' | 'agent_behavior' | 'user_impact' | 'data_integrity' | 'performance'
  severity: 'info' | 'warning' | 'error' | 'critical'
  description: string
  metrics: Record<string, number>
  recommendation?: string
}

export interface RollbackEvent {
  timestamp: number
  trigger: string
  action: string
  success: boolean
  recoveryTime: number
}

export interface SafeguardActivation {
  safeguardId: string
  timestamp: number
  trigger: ChaosCondition
  action: string
  effectiveness: number // 0-100 percentage
}

// ===================== CHAOS ENGINEERING ENGINE =====================

export class ChaosEngineeringEngine extends EventEmitter {
  private activeExperiments: Map<string, ChaosExperiment> = new Map()
  private activeFaults: Map<string, ChaosFaultInjector> = new Map()
  private systemMonitor: ChaosSystemMonitor
  private safeguardManager: ChaosSafeguardManager
  private isRunning: boolean = false

  constructor() {
    super()
    this.systemMonitor = new ChaosSystemMonitor()
    this.safeguardManager = new ChaosSafeguardManager()
  }

  // ===================== EXPERIMENT EXECUTION =====================

  async runExperiment(experiment: ChaosExperiment): Promise<ChaosExperimentResult> {
    console.log(`üî• Starting chaos experiment: ${experiment.name}`)
    console.log(`  Hypothesis: ${experiment.hypothesis}`)
    console.log(`  Blast Radius: ${experiment.blastRadius}`)
    console.log(`  Severity: ${experiment.severity}`)

    const experimentId = experiment.id
    const startTime = performance.now()

    try {
      // Validate experiment safety
      await this.validateExperimentSafety(experiment)

      // Store active experiment
      this.activeExperiments.set(experimentId, experiment)
      this.isRunning = true

      // Setup safeguards
      await this.setupSafeguards(experiment)

      // Start system monitoring
      await this.systemMonitor.startMonitoring(experiment.metrics)

      // Execute experiment phases
      const result = await this.executeExperimentPhases(experiment, startTime)

      console.log(`‚úÖ Chaos experiment completed: ${result.success ? 'SUCCESS' : 'FAILED'}`)
      return result

    } catch (error) {
      console.error(`‚ùå Chaos experiment failed:`, error)
      throw error
    } finally {
      // Cleanup
      await this.cleanupExperiment(experimentId)
      this.isRunning = false
    }
  }

  private async executeExperimentPhases(
    experiment: ChaosExperiment,
    startTime: number
  ): Promise<ChaosExperimentResult> {
    const phases = [
      { name: 'baseline', duration: 30000 }, // 30 seconds baseline
      { name: 'fault_injection', duration: experiment.duration },
      { name: 'recovery', duration: 60000 } // 60 seconds recovery observation
    ]

    const faultResults: ChaosFaultResult[] = []
    const observations: ChaosObservation[] = []
    const rollbackEvents: RollbackEvent[] = []
    const safeguardActivations: SafeguardActivation[] = []

    // Phase 1: Establish Baseline
    console.log('üìä Phase 1: Establishing baseline metrics...')
    await this.establishBaseline(experiment)

    // Phase 2: Inject Faults
    console.log('üí• Phase 2: Injecting faults...')
    const faultPromises = experiment.faults.map(fault => 
      this.injectFault(fault, experiment)
    )

    const faultInjectionResults = await Promise.allSettled(faultPromises)
    
    for (let i = 0; i < faultInjectionResults.length; i++) {
      const result = faultInjectionResults[i]
      if (result.status === 'fulfilled') {
        faultResults.push(result.value)
      } else {
        console.error(`Fault injection failed:`, result.reason)
        observations.push({
          timestamp: Date.now(),
          category: 'system_behavior',
          severity: 'error',
          description: `Fault injection failed: ${result.reason}`,
          metrics: {}
        })
      }
    }

    // Monitor for rollback triggers during fault injection
    const rollbackPromise = this.monitorRollbackTriggers(experiment)

    // Wait for fault injection duration
    await new Promise(resolve => setTimeout(resolve, experiment.duration))

    // Phase 3: Recovery Observation
    console.log('üîÑ Phase 3: Observing recovery...')
    await this.stopAllFaults()
    await this.observeRecovery(experiment)

    // Collect final metrics
    const endTime = performance.now()
    const systemMetrics = await this.systemMonitor.getMetrics()

    // Evaluate success criteria
    const success = this.evaluateSuccessCriteria(experiment, systemMetrics)

    return {
      experimentId: experiment.id,
      startTime,
      endTime,
      duration: endTime - startTime,
      success,
      faultResults,
      systemMetrics,
      observations,
      rollbackEvents,
      safeguardActivations,
      lessons: this.extractLessons(systemMetrics, observations),
      recommendations: this.generateRecommendations(systemMetrics, observations)
    }
  }

  // ===================== FAULT INJECTION =====================

  private async injectFault(fault: ChaosFault, experiment: ChaosExperiment): Promise<ChaosFaultResult> {
    console.log(`  üéØ Injecting fault: ${fault.type} on ${fault.target.identifier}`)

    const injector = this.createFaultInjector(fault)
    this.activeFaults.set(fault.id, injector)

    const startTime = performance.now()
    await injector.inject()

    // Monitor fault effectiveness
    const monitoringPromise = this.monitorFaultEffectiveness(fault, injector)

    // Wait for fault duration
    await new Promise(resolve => setTimeout(resolve, fault.duration))

    // Stop fault injection
    await injector.stop()
    const endTime = performance.now()

    const effectivenessMetrics = await monitoringPromise

    return {
      faultId: fault.id,
      injectionStartTime: startTime,
      injectionEndTime: endTime,
      actualIntensity: effectivenessMetrics.actualIntensity,
      targetCompliance: effectivenessMetrics.targetCompliance,
      impactMetrics: effectivenessMetrics.impactMetrics,
      unexpectedEffects: effectivenessMetrics.unexpectedEffects
    }
  }

  private createFaultInjector(fault: ChaosFault): ChaosFaultInjector {
    switch (fault.type) {
      case 'latency':
        return new LatencyFaultInjector(fault)
      case 'failure':
        return new FailureFaultInjector(fault)
      case 'resource_exhaustion':
        return new ResourceExhaustionInjector(fault)
      case 'network_partition':
        return new NetworkPartitionInjector(fault)
      case 'memory_leak':
        return new MemoryLeakInjector(fault)
      case 'cpu_spike':
        return new CPUSpikeInjector(fault)
      case 'timeout':
        return new TimeoutInjector(fault)
      default:
        return new GenericFaultInjector(fault)
    }
  }

  // ===================== SYSTEM MONITORING =====================

  private async establishBaseline(experiment: ChaosExperiment): Promise<void> {
    await this.systemMonitor.establishBaseline(30000) // 30 seconds
  }

  private async observeRecovery(experiment: ChaosExperiment): Promise<void> {
    await this.systemMonitor.observeRecovery(60000) // 60 seconds
  }

  private async monitorRollbackTriggers(experiment: ChaosExperiment): Promise<void> {
    for (const trigger of experiment.rollbackTriggers) {
      this.systemMonitor.on('metric-threshold-breached', (metricData) => {
        if (this.shouldTriggerRollback(trigger, metricData)) {
          console.warn(`üö® Rollback trigger activated: ${trigger.condition.metric}`)
          this.executeRollbackAction(trigger, experiment)
        }
      })
    }
  }

  private shouldTriggerRollback(trigger: RollbackTrigger, metricData: any): boolean {
    const condition = trigger.condition
    const value = metricData[condition.metric]
    
    if (value === undefined) return false

    switch (condition.operator) {
      case '>': return value > condition.threshold
      case '<': return value < condition.threshold
      case '=': return value === condition.threshold
      case '>=': return value >= condition.threshold
      case '<=': return value <= condition.threshold
      case '!=': return value !== condition.threshold
      default: return false
    }
  }

  private async executeRollbackAction(trigger: RollbackTrigger, experiment: ChaosExperiment): Promise<void> {
    switch (trigger.action) {
      case 'stop_experiment':
        await this.stopAllFaults()
        break
      case 'reduce_intensity':
        await this.reduceAllFaultIntensity(50) // Reduce by 50%
        break
      case 'activate_safeguard':
        await this.safeguardManager.activateEmergencySafeguards()
        break
    }

    this.emit('rollback-triggered', {
      trigger: trigger.condition.metric,
      action: trigger.action,
      timestamp: Date.now()
    })
  }

  // ===================== SAFEGUARD MANAGEMENT =====================

  private async setupSafeguards(experiment: ChaosExperiment): Promise<void> {
    for (const safeguard of experiment.safeguards) {
      await this.safeguardManager.setupSafeguard(safeguard)
    }
  }

  // ===================== EVALUATION AND ANALYSIS =====================

  private evaluateSuccessCriteria(
    experiment: ChaosExperiment,
    metrics: ChaosSystemMetrics
  ): boolean {
    const criteria = experiment.successCriteria

    const checks = [
      metrics.availability >= criteria.systemAvailability,
      metrics.responseTime.increase <= criteria.maxResponseTimeIncrease,
      metrics.errorRate.increase <= criteria.maxErrorRateIncrease,
      metrics.recoveryMetrics.fullRecoveryTime <= criteria.recoveryTime
    ]

    return checks.every(check => check)
  }

  private extractLessons(metrics: ChaosSystemMetrics, observations: ChaosObservation[]): string[] {
    const lessons: string[] = []

    // Analyze response time impact
    if (metrics.responseTime.increase > 50) {
      lessons.push('System response time significantly degraded under fault conditions')
    }

    // Analyze recovery capabilities
    if (metrics.recoveryMetrics.detectionTime > 30000) {
      lessons.push('Fault detection could be improved - took longer than 30 seconds')
    }

    // Analyze error patterns
    const errorObservations = observations.filter(o => o.severity === 'error')
    if (errorObservations.length > 5) {
      lessons.push('High number of error conditions observed during experiment')
    }

    // Analyze resource impact
    if (metrics.resourceUsage.memory.peak > metrics.resourceUsage.memory.baseline * 2) {
      lessons.push('Memory usage spiked significantly - potential memory leak or inefficient handling')
    }

    return lessons
  }

  private generateRecommendations(
    metrics: ChaosSystemMetrics,
    observations: ChaosObservation[]
  ): string[] {
    const recommendations: string[] = []

    // Recovery time recommendations
    if (metrics.recoveryMetrics.fullRecoveryTime > 60000) {
      recommendations.push('Implement faster recovery mechanisms or improve monitoring alerting')
    }

    // Performance recommendations
    if (metrics.responseTime.increase > 100) {
      recommendations.push('Add circuit breakers or fallback mechanisms to prevent cascading failures')
    }

    // Resource recommendations
    if (metrics.resourceUsage.cpu.peak > 90) {
      recommendations.push('Implement CPU throttling or auto-scaling to handle resource spikes')
    }

    // Error handling recommendations
    const criticalObservations = observations.filter(o => o.severity === 'critical')
    if (criticalObservations.length > 0) {
      recommendations.push('Improve error handling and graceful degradation for critical components')
    }

    return recommendations
  }

  // ===================== UTILITY METHODS =====================

  private async validateExperimentSafety(experiment: ChaosExperiment): Promise<void> {
    // Check blast radius
    if (experiment.blastRadius === 'system_wide' && experiment.severity === 'critical') {
      throw new Error('System-wide critical experiments require additional approval')
    }

    // Check safeguard coverage
    if (experiment.severity === 'high' || experiment.severity === 'critical') {
      if (experiment.safeguards.length === 0) {
        throw new Error('High/critical severity experiments require safeguards')
      }
    }

    // Validate rollback triggers
    if (experiment.rollbackTriggers.length === 0) {
      console.warn('‚ö†Ô∏è  No rollback triggers defined - experiment may be risky')
    }
  }

  private async monitorFaultEffectiveness(
    fault: ChaosFault,
    injector: ChaosFaultInjector
  ): Promise<{
    actualIntensity: number
    targetCompliance: number
    impactMetrics: Record<string, number>
    unexpectedEffects: string[]
  }> {
    // Monitor the actual effectiveness of fault injection
    return {
      actualIntensity: fault.intensity * 0.9, // Simulate 90% effectiveness
      targetCompliance: 90,
      impactMetrics: {
        'response_time_increase': 150,
        'error_rate_increase': 5,
        'throughput_decrease': 20
      },
      unexpectedEffects: []
    }
  }

  private async stopAllFaults(): Promise<void> {
    const stopPromises = Array.from(this.activeFaults.values()).map(injector => 
      injector.stop()
    )
    
    await Promise.allSettled(stopPromises)
    this.activeFaults.clear()
  }

  private async reduceAllFaultIntensity(reductionPercent: number): Promise<void> {
    for (const injector of this.activeFaults.values()) {
      await injector.reduceIntensity(reductionPercent)
    }
  }

  private async cleanupExperiment(experimentId: string): Promise<void> {
    // Stop all faults
    await this.stopAllFaults()

    // Stop monitoring
    await this.systemMonitor.stop()

    // Cleanup safeguards
    await this.safeguardManager.cleanup()

    // Remove from active experiments
    this.activeExperiments.delete(experimentId)
  }

  // ===================== PUBLIC API =====================

  getActiveExperiments(): string[] {
    return Array.from(this.activeExperiments.keys())
  }

  async stopExperiment(experimentId: string): Promise<void> {
    if (this.activeExperiments.has(experimentId)) {
      await this.cleanupExperiment(experimentId)
    }
  }

  isExperimentRunning(): boolean {
    return this.isRunning
  }
}

// ===================== FAULT INJECTORS =====================

abstract class ChaosFaultInjector {
  protected fault: ChaosFault
  protected isActive: boolean = false

  constructor(fault: ChaosFault) {
    this.fault = fault
  }

  abstract inject(): Promise<void>
  abstract stop(): Promise<void>
  abstract reduceIntensity(percent: number): Promise<void>

  isInjectionActive(): boolean {
    return this.isActive
  }
}

class LatencyFaultInjector extends ChaosFaultInjector {
  private latencyInterceptor: any = null

  async inject(): Promise<void> {
    this.isActive = true
    const latencyMs = this.fault.parameters.latencyMs || 1000

    // Mock latency injection
    this.latencyInterceptor = setInterval(() => {
      if (this.isActive) {
        // Simulate latency injection
        console.log(`  üí§ Injecting ${latencyMs}ms latency to ${this.fault.target.identifier}`)
      }
    }, 5000)
  }

  async stop(): Promise<void> {
    this.isActive = false
    if (this.latencyInterceptor) {
      clearInterval(this.latencyInterceptor)
      this.latencyInterceptor = null
    }
  }

  async reduceIntensity(percent: number): Promise<void> {
    if (this.fault.parameters.latencyMs) {
      this.fault.parameters.latencyMs *= (1 - percent / 100)
    }
  }
}

class FailureFaultInjector extends ChaosFaultInjector {
  private failureInterceptor: any = null

  async inject(): Promise<void> {
    this.isActive = true
    const errorRate = this.fault.parameters.errorRate || 50

    // Mock failure injection
    this.failureInterceptor = setInterval(() => {
      if (this.isActive && Math.random() * 100 < errorRate) {
        console.log(`  üí• Injecting failure to ${this.fault.target.identifier}`)
      }
    }, 2000)
  }

  async stop(): Promise<void> {
    this.isActive = false
    if (this.failureInterceptor) {
      clearInterval(this.failureInterceptor)
      this.failureInterceptor = null
    }
  }

  async reduceIntensity(percent: number): Promise<void> {
    if (this.fault.parameters.errorRate) {
      this.fault.parameters.errorRate *= (1 - percent / 100)
    }
  }
}

class ResourceExhaustionInjector extends ChaosFaultInjector {
  private exhaustionProcess: any = null

  async inject(): Promise<void> {
    this.isActive = true
    
    // Mock resource exhaustion
    console.log(`  üî• Starting resource exhaustion on ${this.fault.target.identifier}`)
    
    // Simulate memory or CPU exhaustion
    if (this.fault.target.type === 'memory') {
      this.exhaustMemory()
    } else if (this.fault.target.type === 'cpu') {
      this.exhaustCPU()
    }
  }

  private exhaustMemory(): void {
    const memoryHogs: Buffer[] = []
    this.exhaustionProcess = setInterval(() => {
      if (this.isActive) {
        // Allocate memory to simulate exhaustion
        memoryHogs.push(Buffer.alloc(1024 * 1024)) // 1MB chunks
        if (memoryHogs.length > 100) { // Limit to 100MB
          memoryHogs.splice(0, 50) // Remove some to prevent actual crash
        }
      }
    }, 100)
  }

  private exhaustCPU(): void {
    this.exhaustionProcess = setInterval(() => {
      if (this.isActive) {
        // CPU-intensive loop
        const start = Date.now()
        while (Date.now() - start < 50) {
          Math.random() * Math.random()
        }
      }
    }, 10)
  }

  async stop(): Promise<void> {
    this.isActive = false
    if (this.exhaustionProcess) {
      clearInterval(this.exhaustionProcess)
      this.exhaustionProcess = null
    }
  }

  async reduceIntensity(percent: number): Promise<void> {
    // Reduce resource exhaustion intensity
    this.fault.intensity *= (1 - percent / 100)
  }
}

class NetworkPartitionInjector extends ChaosFaultInjector {
  async inject(): Promise<void> {
    this.isActive = true
    console.log(`  üåê Simulating network partition for ${this.fault.target.identifier}`)
    // Mock network partition - in real implementation, this would block network calls
  }

  async stop(): Promise<void> {
    this.isActive = false
    console.log(`  üåê Restoring network connectivity for ${this.fault.target.identifier}`)
  }

  async reduceIntensity(percent: number): Promise<void> {
    // Reduce partition severity
    this.fault.intensity *= (1 - percent / 100)
  }
}

class MemoryLeakInjector extends ChaosFaultInjector {
  private leakInterval: any = null
  private leakedMemory: Buffer[] = []

  async inject(): Promise<void> {
    this.isActive = true
    const leakRate = this.fault.parameters.memoryLeakRate || 1 // MB per second

    this.leakInterval = setInterval(() => {
      if (this.isActive) {
        const leakSize = leakRate * 1024 * 1024 // Convert to bytes
        this.leakedMemory.push(Buffer.alloc(leakSize))
        console.log(`  üï≥Ô∏è  Memory leak: ${this.leakedMemory.length * leakRate}MB leaked`)
      }
    }, 1000)
  }

  async stop(): Promise<void> {
    this.isActive = false
    if (this.leakInterval) {
      clearInterval(this.leakInterval)
      this.leakInterval = null
    }
    this.leakedMemory = [] // Release memory
  }

  async reduceIntensity(percent: number): Promise<void> {
    if (this.fault.parameters.memoryLeakRate) {
      this.fault.parameters.memoryLeakRate *= (1 - percent / 100)
    }
  }
}

class CPUSpikeInjector extends ChaosFaultInjector {
  private spikeInterval: any = null

  async inject(): Promise<void> {
    this.isActive = true
    const cpuPercent = this.fault.parameters.cpuUsagePercent || 80

    this.spikeInterval = setInterval(() => {
      if (this.isActive) {
        // Create CPU spike
        const duration = (cpuPercent / 100) * 100 // Scale duration with percentage
        const start = Date.now()
        while (Date.now() - start < duration) {
          Math.random() * Math.random() * Math.random()
        }
      }
    }, 100)
  }

  async stop(): Promise<void> {
    this.isActive = false
    if (this.spikeInterval) {
      clearInterval(this.spikeInterval)
      this.spikeInterval = null
    }
  }

  async reduceIntensity(percent: number): Promise<void> {
    if (this.fault.parameters.cpuUsagePercent) {
      this.fault.parameters.cpuUsagePercent *= (1 - percent / 100)
    }
  }
}

class TimeoutInjector extends ChaosFaultInjector {
  async inject(): Promise<void> {
    this.isActive = true
    console.log(`  ‚è∞ Injecting timeouts for ${this.fault.target.identifier}`)
    // Mock timeout injection - would intercept and delay responses
  }

  async stop(): Promise<void> {
    this.isActive = false
    console.log(`  ‚è∞ Removing timeout injection for ${this.fault.target.identifier}`)
  }

  async reduceIntensity(percent: number): Promise<void> {
    if (this.fault.parameters.timeoutMs) {
      this.fault.parameters.timeoutMs *= (1 - percent / 100)
    }
  }
}

class GenericFaultInjector extends ChaosFaultInjector {
  async inject(): Promise<void> {
    this.isActive = true
    console.log(`  ‚ö° Injecting generic fault: ${this.fault.type}`)
  }

  async stop(): Promise<void> {
    this.isActive = false
    console.log(`  ‚ö° Stopping generic fault: ${this.fault.type}`)
  }

  async reduceIntensity(percent: number): Promise<void> {
    this.fault.intensity *= (1 - percent / 100)
  }
}

// ===================== SYSTEM MONITOR =====================

class ChaosSystemMonitor extends EventEmitter {
  private isMonitoring: boolean = false
  private monitoringInterval: NodeJS.Timeout | null = null
  private baselineMetrics: Record<string, number> = {}
  private currentMetrics: Record<string, number> = {}

  async startMonitoring(metrics: string[]): Promise<void> {
    this.isMonitoring = true
    
    this.monitoringInterval = setInterval(() => {
      if (this.isMonitoring) {
        this.collectMetrics()
      }
    }, 1000)
  }

  async establishBaseline(duration: number): Promise<void> {
    console.log('üìä Establishing baseline metrics...')
    
    // Collect baseline metrics for the specified duration
    const samples: Record<string, number[]> = {}
    const sampleInterval = setInterval(() => {
      const metrics = this.getCurrentSystemMetrics()
      
      for (const [key, value] of Object.entries(metrics)) {
        if (!samples[key]) samples[key] = []
        samples[key].push(value)
      }
    }, 1000)

    await new Promise(resolve => setTimeout(resolve, duration))
    clearInterval(sampleInterval)

    // Calculate baseline averages
    for (const [key, values] of Object.entries(samples)) {
      this.baselineMetrics[key] = values.reduce((sum, val) => sum + val, 0) / values.length
    }

    console.log('‚úÖ Baseline established:', this.baselineMetrics)
  }

  async observeRecovery(duration: number): Promise<void> {
    console.log('üîÑ Observing system recovery...')
    await new Promise(resolve => setTimeout(resolve, duration))
  }

  private collectMetrics(): void {
    this.currentMetrics = this.getCurrentSystemMetrics()
    
    // Check for threshold breaches
    for (const [metric, value] of Object.entries(this.currentMetrics)) {
      const baseline = this.baselineMetrics[metric] || 0
      const increase = baseline > 0 ? ((value - baseline) / baseline) * 100 : 0
      
      if (increase > 50) { // 50% increase threshold
        this.emit('metric-threshold-breached', {
          [metric]: value,
          baseline,
          increase
        })
      }
    }
  }

  private getCurrentSystemMetrics(): Record<string, number> {
    const memoryUsage = process.memoryUsage()
    const cpuUsage = process.cpuUsage()
    
    return {
      memory_used: memoryUsage.heapUsed / 1024 / 1024, // MB
      memory_total: memoryUsage.heapTotal / 1024 / 1024, // MB
      cpu_user: cpuUsage.user / 1000, // milliseconds
      cpu_system: cpuUsage.system / 1000, // milliseconds
      response_time: 100 + Math.random() * 200, // Mock response time
      error_rate: Math.random() * 5, // Mock error rate
      throughput: 50 + Math.random() * 100 // Mock throughput
    }
  }

  async getMetrics(): Promise<ChaosSystemMetrics> {
    const current = this.currentMetrics
    const baseline = this.baselineMetrics

    return {
      availability: 99.5 - Math.random() * 2, // Mock availability
      responseTime: {
        baseline: baseline.response_time || 100,
        duringChaos: current.response_time || 150,
        increase: ((current.response_time || 150) - (baseline.response_time || 100)) / (baseline.response_time || 100) * 100
      },
      errorRate: {
        baseline: baseline.error_rate || 1,
        duringChaos: current.error_rate || 3,
        increase: ((current.error_rate || 3) - (baseline.error_rate || 1)) / (baseline.error_rate || 1) * 100
      },
      throughput: {
        baseline: baseline.throughput || 100,
        duringChaos: current.throughput || 80,
        decrease: ((baseline.throughput || 100) - (current.throughput || 80)) / (baseline.throughput || 100) * 100
      },
      resourceUsage: {
        memory: {
          baseline: baseline.memory_used || 100,
          peak: Math.max(current.memory_used || 150, baseline.memory_used || 100)
        },
        cpu: {
          baseline: baseline.cpu_user || 20,
          peak: Math.max(current.cpu_user || 40, baseline.cpu_user || 20)
        },
        network: {
          baseline: 1000,
          peak: 1500
        }
      },
      recoveryMetrics: {
        detectionTime: 15000, // Mock 15 seconds
        mitigationTime: 30000, // Mock 30 seconds
        fullRecoveryTime: 45000 // Mock 45 seconds
      }
    }
  }

  async stop(): Promise<void> {
    this.isMonitoring = false
    if (this.monitoringInterval) {
      clearInterval(this.monitoringInterval)
      this.monitoringInterval = null
    }
  }
}

// ===================== SAFEGUARD MANAGER =====================

class ChaosSafeguardManager {
  private activeSafeguards: Map<string, ChaosSafeguard> = new Map()

  async setupSafeguard(safeguard: ChaosSafeguard): Promise<void> {
    this.activeSafeguards.set(safeguard.id, safeguard)
    console.log(`üõ°Ô∏è  Safeguard setup: ${safeguard.type}`)
  }

  async activateEmergencySafeguards(): Promise<void> {
    console.log('üö® Activating emergency safeguards!')
    
    for (const safeguard of this.activeSafeguards.values()) {
      if (safeguard.type === 'emergency_stop') {
        await this.activateSafeguard(safeguard)
      }
    }
  }

  private async activateSafeguard(safeguard: ChaosSafeguard): Promise<void> {
    console.log(`üõ°Ô∏è  Activating safeguard: ${safeguard.type}`)
    // Implementation would execute the safeguard action
  }

  async cleanup(): Promise<void> {
    this.activeSafeguards.clear()
  }
}

// ===================== PREDEFINED CHAOS EXPERIMENTS =====================

export const PredefinedChaosExperiments: ChaosExperiment[] = [
  {
    id: 'agent-latency-resilience',
    name: 'Agent Latency Resilience Test',
    description: 'Test system resilience when AI agents experience increased latency',
    hypothesis: 'The system should maintain acceptable performance when agent response times increase by 300%',
    blastRadius: 'agent_group',
    severity: 'medium',
    duration: 300000, // 5 minutes
    faults: [
      {
        id: 'agent-latency-fault',
        type: 'latency',
        target: {
          type: 'agent',
          identifier: 'pattern-discovery-agent',
          scope: 'specific'
        },
        parameters: {
          latencyMs: 3000
        },
        duration: 300000,
        intensity: 100,
        injectionPattern: 'constant',
        conditions: []
      }
    ],
    successCriteria: {
      systemAvailability: 95,
      maxResponseTimeIncrease: 200,
      maxErrorRateIncrease: 10,
      recoveryTime: 60000,
      dataIntegrity: true,
      agentCoordination: true,
      emergencyProtocols: true
    },
    rollbackTriggers: [
      {
        condition: {
          metric: 'error_rate',
          operator: '>',
          threshold: 25,
          evaluationPeriod: 30000
        },
        action: 'stop_experiment',
        priority: 'high'
      }
    ],
    safeguards: [
      {
        id: 'circuit-breaker-safeguard',
        type: 'circuit_breaker',
        trigger: {
          metric: 'response_time',
          operator: '>',
          threshold: 5000,
          evaluationPeriod: 10000
        },
        action: 'activate_circuit_breaker',
        autoActivate: true
      }
    ],
    metrics: ['response_time', 'error_rate', 'memory_used', 'cpu_user', 'throughput']
  },
  {
    id: 'database-connection-failure',
    name: 'Database Connection Failure Test',
    description: 'Test system behavior when database connections fail intermittently',
    hypothesis: 'The system should gracefully handle database failures and recover automatically',
    blastRadius: 'external_dependencies',
    severity: 'high',
    duration: 180000, // 3 minutes
    faults: [
      {
        id: 'db-connection-fault',
        type: 'failure',
        target: {
          type: 'database',
          identifier: 'primary-db',
          scope: 'specific'
        },
        parameters: {
          errorRate: 30,
          errorMessage: 'Connection timeout'
        },
        duration: 180000,
        intensity: 30,
        injectionPattern: 'intermittent',
        conditions: []
      }
    ],
    successCriteria: {
      systemAvailability: 90,
      maxResponseTimeIncrease: 150,
      maxErrorRateIncrease: 15,
      recoveryTime: 30000,
      dataIntegrity: true,
      agentCoordination: true,
      emergencyProtocols: true
    },
    rollbackTriggers: [
      {
        condition: {
          metric: 'availability',
          operator: '<',
          threshold: 80,
          evaluationPeriod: 60000
        },
        action: 'stop_experiment',
        priority: 'emergency'
      }
    ],
    safeguards: [
      {
        id: 'fallback-database',
        type: 'fallback',
        trigger: {
          metric: 'error_rate',
          operator: '>',
          threshold: 50,
          evaluationPeriod: 30000
        },
        action: 'switch_to_fallback_db',
        autoActivate: true
      }
    ],
    metrics: ['availability', 'response_time', 'error_rate', 'data_integrity']
  }
]

// ===================== FACTORY FUNCTION =====================

export function createChaosExperiment(config: Partial<ChaosExperiment>): ChaosExperiment {
  const defaultConfig: ChaosExperiment = {
    id: `chaos-${Date.now()}`,
    name: 'Default Chaos Experiment',
    description: 'Basic chaos engineering test',
    hypothesis: 'System should maintain stability under fault conditions',
    blastRadius: 'single_agent',
    severity: 'low',
    duration: 60000,
    faults: [],
    successCriteria: {
      systemAvailability: 95,
      maxResponseTimeIncrease: 100,
      maxErrorRateIncrease: 10,
      recoveryTime: 30000,
      dataIntegrity: true,
      agentCoordination: true,
      emergencyProtocols: true
    },
    rollbackTriggers: [],
    safeguards: [],
    metrics: ['response_time', 'error_rate', 'availability']
  }

  return { ...defaultConfig, ...config }
}

export function createChaosEngine(): ChaosEngineeringEngine {
  return new ChaosEngineeringEngine()
}